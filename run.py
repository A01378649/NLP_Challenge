#!/usr/bin/env python
# coding: utf-8

# In[53]:


import tensorflow
import numpy as np
import os
import random
import boto3
import awscli
#!pip install python-dotenv
from dotenv import load_dotenv
import transformers
from transformers.keras_callbacks import KerasMetricCallback
from transformers import pipeline, AutoTokenizer, create_optimizer, DataCollatorWithPadding, TrainingArguments, Trainer, AutoModelForTokenClassification, DataCollatorForTokenClassification, TFAutoModelForTokenClassification
from datasets import load_dataset, load_metric
from nltk.translate.bleu_score import sentence_bleu
import keras
#!pip install google-cloud-translate==3.0.0
from google.cloud import translate



"""
For tests, best practices are to have a structure like this: 
https://stackoverflow.com/questions/1896918/running-unittest-with-typical-test-directory-structure

so go ahead and separate the tests from the other logic, since when deploying, you don't want the testing logic mixed in with the main logic! 


Part of this homework is the ability to write ML code in industry, which can’t be just coming from python notebooks! 
It’s best practice to have your code in a descriptive method or small class, if possible, rather than running at the top-level. 
This makes it easier for other modules to import the functionality later if needed! 

Try to split your code into files, methods, and classes with descriptive names! Some reading for this: 

Classes: https://www.dataquest.io/blog/using-classes-in-python/ and https://www.geeksforgeeks.org/python-classes-and-objects/

"""
# In[2]:


#PART 1
print("Part 1:\n-------------------Out of the Box Sentiment Analysis----------------------")


# In[3]:


classifier = pipeline("sentiment-analysis")


# In[4]:


#Extracting the reviews ot txt into a list...
REVIEWS_PATH = "tiny_movie_reviews_dataset.txt"

def get_movie_reviews(path):
    with open(path, "r") as f:
        return f.readlines()
    

reviews = get_movie_reviews(REVIEWS_PATH)

#The 18th review is too long so we truncate its beginning elements
reviews[17] = reviews[17][259:]


# In[5]:


#We introduce each review into the pipeline and extract the result
def print_sentiment(reviews):
    results = classifier(reviews)
    
    for result in results:
        print(result['label'])
        
    return True
        
print_sentiment(reviews)


# In[6]:


#PART 2
print("Part 2:\n-------------------NER----------------------")
'''
This is a tweaked version of this notebook: https://github.com/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb
'''

#We are going to use the "distilled" (light) version of the multilingual cased BERT transformer
model_checkpoint = "distilbert-base-uncased"
batch_size = 10                                        #Batch size has to be this low because the gpu can't handle any bigger batch sizes


#WikiANN dataset consists of Wikipedia articles with their respective NER annotations
from datasets import load_dataset, load_metric
datasets = load_dataset("wikiann","en")                                      #We are going to choose the english subset of the database


# In[7]:


#Initializing tokenizer to generate features important to the transformer
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)


# In[8]:


'''
A workaround to solve the length mismatch between the inital labels and the input_ids generated by the tokenizer
In simple terms, we extract the label that corresponds (should) to each word id generated by the tokenizer from the "ner_tags" and place it in other list 
The function is modified slightly to simplify operations (deleted some ifs)
'''

def tokenize_and_align_labels(examples):
    tokenized_inputs = tokenizer(
        examples["tokens"], truncation=True, is_split_into_words=True
    )

    labels = []
    for i, label in enumerate(examples["ner_tags"]):
        word_ids = tokenized_inputs.word_ids(batch_index=i)
        previous_word_idx = None
        label_ids = []
        for word_idx in word_ids:
            # Special tokens have a word id that is None. We set the label to -100 so they are automatically
            # ignored in the loss function.
            if word_idx is None:
                label_ids.append(-100)
            else:
                label_ids.append(label[word_idx])

        labels.append(label_ids)

    tokenized_inputs["labels"] = labels
    return tokenized_inputs


# In[9]:


#Generating new features for transformer processing
tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)


# In[10]:


#List of all tags
label_list = datasets["train"].features["ner_tags"].feature.names
#Dictionary of ids pointing to their corresponding label (defined for cleaner output)
id2label = {i: label for i, label in enumerate(label_list)}
#Dictionary of labels pointing to their corresponding id (defined for cleaner output)
label2id = {label: i for i, label in enumerate(label_list)}

#We define a Tensorflow model based on the same model as the tokenizer
model = TFAutoModelForTokenClassification.from_pretrained(
    model_checkpoint, num_labels=len(label_list), id2label=id2label, label2id=label2id
)


# In[11]:


N_EPOCHS = 1
INIT_LR = 2e-5

#Create an optimizer without defining loss because the metrics will be calculated with a callback later on
def get_tf_optimizer(epochs, lr):
    num_train_steps = (len(tokenized_datasets["train"]) // batch_size) * epochs
    
    optimizer, lr_schedule = create_optimizer(
    init_lr=2e-5,
    num_train_steps=num_train_steps,
    weight_decay_rate=0.01,
    num_warmup_steps=0)
    
    return optimizer


# In[12]:


type(get_tf_optimizer(N_EPOCHS, INIT_LR))


# In[13]:


model.compile(optimizer=get_tf_optimizer(N_EPOCHS, INIT_LR))


# In[14]:


#Collator for data preprocessing like adding padding or even some data augmentation for each batch
data_collator = DataCollatorForTokenClassification(tokenizer, return_tensors="tf")


# In[15]:


TRAIN_PERCENTAGE = 0.1
VALIDATION_PERCENTAGE = 0.1
TEST_PERCENTAGE = 0.1

#Get dataset splits for training, validation and test
def get_model_splits(model, datasets, data_collator, batch_size=8, tr_percent=0.1, v_percent=0.1, te_percent=0.1):
    n_train = int(len(datasets["train"]) * tr_percent)
    n_validation = int(len(datasets["validation"]) * v_percent)
    n_test = int(len(datasets["test"]) * te_percent)
    
    return[model.prepare_tf_dataset(
        datasets["train"].select(range(n_train)),
        shuffle=True,
        batch_size=batch_size,
        collate_fn=data_collator
        ),

    model.prepare_tf_dataset(
        datasets["validation"].select(range(n_validation)),
        shuffle=True,
        batch_size=batch_size,
        collate_fn=data_collator
        ),

    model.prepare_tf_dataset(
        datasets["test"].select(range(n_test)),
        shuffle=True,
        batch_size=batch_size,
        collate_fn=data_collator
        )]


# In[16]:


#Initialize splits
train_set, validation_set, test_set = get_model_splits(model, tokenized_datasets, data_collator)


# In[17]:


#We define a class that will store the metrics for each set and instantiate the objects

class Metrics:
    def __init__(self):
      self.accuracy = []
      self.precision = []
      self.f1 = []
      self.recall = []

    def update(self, metrics):
      self.accuracy.append(metrics["accuracy"])
      self.precision.append(metrics["precision"])
      self.f1.append(metrics["f1"])
      self.recall.append(metrics["recall"])
        
train_metrics = Metrics()
validation_metrics = Metrics()
test_metrics = Metrics()


# In[18]:


'''
This method will use seqeval method to compute metrics of the model using a specified dataset split (defined in a later function)
The tags for special tokens will be removed.
This function was mostly unchanged from the source.
'''

def compute_metrics(p):
    metric = load_metric("seqeval")
    
    predictions, labels = p
    predictions = np.argmax(predictions, axis=2)

    # Remove ignored index (special tokens)
    true_predictions = [
        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]
    true_labels = [
        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]
        for prediction, label in zip(predictions, labels)
    ]

    results = metric.compute(predictions=true_predictions, references=true_labels)

    result_dc = {
        "precision": results["overall_precision"],
        "recall": results["overall_recall"],
        "f1": results["overall_f1"],
        "accuracy": results["overall_accuracy"],
    }

    return result_dc


# In[19]:


#We define the callbacks using auxiliary functions to update each metrics object
def update_train_metrics(p):
    metrics = compute_metrics(p)
    train_metrics.update(metrics)
    return metrics

def update_validation_metrics(p):
    metrics = compute_metrics(p)
    validation_metrics.update(metrics)
    return metrics

def update_test_metrics(p):
    metrics = compute_metrics(p)
    test_metrics.update(metrics)
    return metrics


# In[20]:


#Instantiating keras callbacks for each split...
metric_train_callback = KerasMetricCallback(
    metric_fn=update_train_metrics, eval_dataset=train_set
)

metric_validation_callback = KerasMetricCallback(
    metric_fn=update_validation_metrics, eval_dataset=validation_set
)

metric_test_callback = KerasMetricCallback(
    metric_fn=update_test_metrics, eval_dataset=test_set
)


# In[21]:


#As previous callbacks can only be executed per epoch, we will define another metrics class that will store batch history
#Keras only makes accesible accuracy and loss via the logs object
class LossHistory(keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self.losses = []
        self.accuracy = []

    def on_batch_end(self, batch, logs={}):
        self.losses.append(logs.get('loss'))
        self.accuracy.append(logs.get('accuracy'))

  

batch_history = LossHistory()


# In[22]:


#Putting it all together to train the model

callbacks = [metric_train_callback, metric_validation_callback, metric_test_callback, batch_history]

history = model.fit(
    train_set,
    validation_data=validation_set,
    epochs=N_EPOCHS,
    callbacks=callbacks
)


# In[23]:


import matplotlib.pyplot as plt

#Plotting loss per batch
plt.xlabel("Batch #")
plt.ylabel("Loss")
plt.title("Loss variation")

plt.plot(batch_history.losses, linewidth=1, color='green')
plt.show()

#Plotting accuracy for the three splits (more than 1 epoch to be appreciated)
plt.xlabel("Epoch #")
plt.ylabel("Accuracy")
plt.title("Split comparison")

plt.plot(train_metrics.accuracy, linewidth=3, color='blue', label = "Train")
plt.plot(validation_metrics.accuracy, linewidth=2, color='yellow', label = "Validation")
plt.plot(test_metrics.accuracy, linewidth=3, color='red', label = "Test")

plt.legend()

plt.show()


# In[24]:


#PART 3
print("Part 3:\n-------------------Translation API's comparison----------------------")


# In[25]:


#Load .env variables
load_dotenv()

#Change this variable to your Google api key location
JSON_PATH = os.getenv('GOOGLE_JSON_PATH')
#Change this variable to your Google project ID
PROJECT_ID = os.getenv('GOOGLE_PROJECT_ID')
parent = f"projects/{PROJECT_ID}"
os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = JSON_PATH

en_path = "dataset_en.txt"                 #English dataset 
es_path = "dataset_es.txt"                 #Spanish dataset


# In[26]:


#Read txt files with proper encoding
def read_english(en_path):
    with open(en_path, "r", encoding="utf-8") as f:
        f_en_cont = f.read()
        
    return f_en_cont.split("\n")
    
def read_spanish(es_path):
    with open(es_path, "r", encoding="iso8859") as f:
        f_es_cont = f.read()
        
    return f_es_cont.split("\n")
    

en_sentences = read_english(en_path)
es_sentences = read_spanish(es_path)


# In[27]:

# constants shuold be at top of file 

N_SENTENCES = 100                     #Number of samples to take

#Generates a preprocessed pair of lists. Due to constraints of the BLEU score function, the reference dataset needs to be split futher into words for each sentence.
#Reference is the dataset whose language is intended to be evaluated with the BLUE score. Source language is the dataset to be translated.
def get_subset_pair(reference, source, n_sentences):
    n = len(en_sentences)
    
    reference_sub = []
    source_sub = []

    for i in range(n_sentences):
        index = random.randrange(n)
        reference_sub.append(reference[index].split())
        source_sub.append(source[index])
        
    return [reference_sub, source_sub]

en_subset, es_subset = get_subset_pair(en_sentences, es_sentences, N_SENTENCES)


# In[28]:


#Uses google's API to return a list of translated sentences
def google_translate_sentence(texts, target):
    client = translate.TranslationServiceClient()
    response = client.translate_text(contents=texts, target_language_code=target, parent=parent)
    translations = []

    for translation in response.translations:
        translations.append(translation.translated_text)    

    return translations

google_translations = google_translate_sentence(es_subset, "en")


# In[29]:


#!pip install boto3
#!pip install awscli
import boto3
import awscli

#Copy-paste keys here
AWS_ACCESS_KEY_ID = os.getenv('AWS_ACCESS_KEY_ID')
AWS_SECRET_ACCESS_KEY = os.getenv('AWS_SECRET_ACCESS_KEY')
AWS_SESSION_TOKEN = os.getenv('AWS_SESSION_TOKEN')

#Creates an AWS session using keys
session = boto3.Session(
    aws_access_key_id=AWS_ACCESS_KEY_ID,
    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,
    aws_session_token=AWS_SESSION_TOKEN
)


# In[30]:


#Uses AWS API to translate a set of texts
def aws_translate_sentence(texts, source, target):
    translate = session.client(service_name='translate', region_name='us-east-1', use_ssl=True)
    translations = []
    
    for text in texts:
        translations.append(translate.translate_text(Text=text, SourceLanguageCode=source, TargetLanguageCode=target).get('TranslatedText'))
                  
    return translations


# In[31]:


aws_translations = aws_translate_sentence(es_subset, "es", "en")


# In[32]:


#The implemented function uses 1-gram BLEU score and computes the average for all provided translations
#The index of a translation must correspond to the index of its reference
def average_bleu_score(references, translations):
    N = len(references)
    acc = 0
    
    for i in range(N):
        acc += sentence_bleu([references[i]], translations[i].split(), weights=(1, 0, 0, 0))
        
    return acc/N


# In[33]:


#Obtaining scores for both APIs
google_score = average_bleu_score(en_subset, google_translations)
aws_score = average_bleu_score(en_subset, aws_translations)


# In[34]:


#Print results
print(f"AMAZON_TRANSLATOR: {aws_score}")
print(f"GOOGLE_TRANSLATOR: {google_score}")


# In[54]:


print("#------------------------------Tests------------------------------\n\n")

def get_movie_reviews_test():
    assert len(get_movie_reviews(REVIEWS_PATH)) == 21
    
def print_sentiment_test():
    assert print_sentiment(reviews) == True
    
def tokenize_test(example):
    tokenized_input = tokenizer(example["tokens"], is_split_into_words=True)
    word_ids = tokenized_input.word_ids()
    aligned_labels = [-100 if i is None else example["ner_tags"][i] for i in word_ids]
    out = datasets.map(tokenize_and_align_labels, batched=True)
    assert len(aligned_labels) == len(out["train"][0]["input_ids"])
    
def get_model_splits_test(model, datasets, data_collator, batch_size=8, tr_percent=0.1, v_percent=0.1, te_percent=0.1):
    tr, v, te = get_model_splits(model, tokenized_datasets, data_collator)
    assert len(tr) == len(datasets["train"]) * tr_percent / batch_size
    
def read_english_test(en_path):
    with open(en_path, "r", encoding="utf-8") as f:
        f_en_cont = f.read()
        
    assert isinstance(f_en_cont.split("\n"), list) == True
    
def read_spanish_test(es_path):
    with open(es_path, "r", encoding="iso8859") as f:
        f_es_cont = f.read()
        
    assert isinstance(f_es_cont.split("\n"), list) == True
    
def get_subset_pair_test(reference, source, n_sentences):
    assert isinstance(get_subset_pair(reference, source, n_sentences), list)
    
def average_bleu_score_test(references, translations):
    assert isinstance(average_bleu_score(references, translations), float)
    
def get_tf_optimizer_test(epochs, lr):
    assert isinstance(get_tf_optimizer(epochs, lr),transformers.optimization_tf.AdamWeightDecay) 
    


# In[59]:


def run_tests():
    get_movie_reviews_test()
    print_sentiment_test()
    tokenize_test(datasets["train"][0])
    get_model_splits_test(model, datasets, data_collator)
    read_english_test(en_path)
    read_spanish_test(es_path)
    get_subset_pair_test(en_sentences, es_sentences, N_SENTENCES)
    average_bleu_score_test(en_subset, aws_translations)
    get_tf_optimizer_test(N_EPOCHS, INIT_LR)
    print("All tests were completed succesfully :^)")


# In[60]:


run_tests()


# In[ ]:




