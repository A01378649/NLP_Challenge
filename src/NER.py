import transformers
from transformers.keras_callbacks import KerasMetricCallback
from transformers import pipeline, AutoTokenizer, create_optimizer, DataCollatorWithPadding, TrainingArguments, Trainer, AutoModelForTokenClassification, DataCollatorForTokenClassification, TFAutoModelForTokenClassification
from datasets import load_dataset, load_metric
import numpy as np
import os

class NER():
    def __init__(self, model_checkpoint):
        self.model_checkpoint = model_checkpoint
        self.tokenizer = None
        self.datasets = None

    #Initializes tokenizer to generate features important to the transformer
    def set_tokenizer(self):
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_checkpoint)

    def load_dataset(self, name, lang):
        self.datasets = load_dataset("wikiann","en")

    '''
    A workaround to solve the length mismatch between the inital labels and the input_ids generated by the tokenizer
    In simple terms, we extract the label that corresponds (should) to each word id generated by the tokenizer from the "ner_tags" and place it in other list 
    The function is modified slightly to simplify operations (deleted some ifs)
    '''
    def tokenize_and_align_labels(self, examples):
        tokenized_inputs = self.tokenizer(
            examples["tokens"], truncation=True, is_split_into_words=True
        )

        labels = []
        for i, label in enumerate(examples["ner_tags"]):
            word_ids = tokenized_inputs.word_ids(batch_index=i)
            previous_word_idx = None
            label_ids = []
            for word_idx in word_ids:
                # Special tokens have a word id that is None. We set the label to -100 so they are automatically
                # ignored in the loss function.
                if word_idx is None:
                    label_ids.append(-100)
                else:
                    label_ids.append(label[word_idx])

            labels.append(label_ids)

        tokenized_inputs["labels"] = labels
        return tokenized_inputs
    
    def get_tokenized_datasets(self):
        return self.datasets.map(self.tokenize_and_align_labels, batched=True)
